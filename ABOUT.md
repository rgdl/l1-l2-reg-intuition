# L1 vs. L2 Regularization
I have a reasonable understanding of L1 and L2 regularization, but not much intuition for how they work. For instance, why does one of them make weights go to zero, essentially resulting in feature selection, but the other doesn't? And which one's which? In this article I'll fit regression models on simulated data to demonstrate the difference, and discuss when you might want to use one instead of the other.

## What Is Regularization?
Penalising large weights to prevent over-fitting

## L1 and L2 Regularization
Same as Lasso and Ridge? Is Elastic Net a combination of both. Why would you do that?
Are these types of regularisation the same thing as weight decay?

## Why Does One Of Them Result in Feature Selection? Show A Code Example

## Why would you choose one over another? Any Implications for Deep Learning?

## Conclusion
